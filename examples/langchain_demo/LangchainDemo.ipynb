{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "3bf01a75-a01b-472e-bc0c-9fe97658eb46",
            "metadata": {},
            "source": [
                "## LlamaIndex <> Langchain Integrations\n",
                "\n",
                "This demo notebook shows how you can provide integrations between LlamaIndex and Langchain. It provides the following examples:\n",
                "- Using LlamaIndex as a callable tool with a Langchain agent\n",
                "- Using LlamaIndex as a memory module; this allows you to insert arbitrary amounts of conversation history with a Langchain chatbot!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "c1568569",
            "metadata": {},
            "outputs": [],
            "source": [
                "from dotenv import find_dotenv, load_dotenv\n",
                "import logging\n",
                "import sys\n",
                "\n",
                "load_dotenv(find_dotenv())\n",
                "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
                "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "bb9177a4-9cb7-4211-b463-121d850b5917",
            "metadata": {},
            "source": [
                "#### Using LlamaIndex as a Callable Tool"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "12af1b0e-983f-4fc1-b5b4-2edeb2e8f07e",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.agents import Tool\n",
                "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
                "from langchain.chat_models import ChatOpenAI\n",
                "from langchain.agents import initialize_agent\n",
                "\n",
                "from llama_index import VectorStoreIndex, SimpleDirectoryReader"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1226132b-2c5f-4073-bfdb-0e36c681c12f",
            "metadata": {},
            "outputs": [],
            "source": [
                "documents = SimpleDirectoryReader(input_files=['/media/sunhuawei/data/Projects/playground/data/paul_graham_essay.txt']).load_data()\n",
                "index = VectorStoreIndex.from_documents(documents=documents)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "3c94a338",
            "metadata": {},
            "outputs": [],
            "source": [
                "index.set_index_id('paul_graham_essay')\n",
                "index.storage_context.persist(persist_dir='/media/sunhuawei/data/Projects/playground/data/storage/pg_essay')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "8c9b3567-c95c-473d-afc0-516b5f35e197",
            "metadata": {},
            "outputs": [],
            "source": [
                "tools = [\n",
                "    Tool(\n",
                "        name = \"LlamaIndex\",\n",
                "        func=lambda q: str(index.as_query_engine().query(q)),\n",
                "        description=\"useful for when you want to answer questions about the author. The input to this tool should be a complete english sentence.\",\n",
                "        return_direct=True\n",
                "    ),\n",
                "]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "374e9f69-1f75-4a62-afdc-22f748d4bddd",
            "metadata": {},
            "outputs": [],
            "source": [
                "# set Logging to DEBUG for more detailed outputs\n",
                "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
                "llm = ChatOpenAI(temperature=0)  # gpt-3.5\n",
                "agent_executor = initialize_agent(tools, llm, agent=\"conversational-react-description\", memory=memory)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "579fbc9f-9f13-416c-bde4-7e56fb899727",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'Hello Bob! How can I assist you today?'"
                        ]
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "agent_executor.run(input=\"hi, i am bob\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "a9841c8e-f90b-4e40-a2f9-ad1e98bb9eef",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
                        "> [retrieve] Total LLM token usage: 0 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 8 tokens\n",
                        "> [retrieve] Total embedding token usage: 8 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1455 tokens\n",
                        "> [get_response] Total LLM token usage: 1455 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
                        "> [get_response] Total embedding token usage: 0 tokens\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "'\\nThe author grew up writing short stories and programming on an IBM 1401. He also nagged his father to buy him a TRS-80 microcomputer, which he used to write simple games, a program to predict how high his model rockets would fly, and a word processor. He also studied philosophy in college, but eventually switched to AI.'"
                        ]
                    },
                    "execution_count": 14,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "agent_executor.run(input=\"What did the author do growing up?\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "7c02eb88-5a4a-4694-9b77-cd46adc691f5",
            "metadata": {},
            "source": [
                "#### Using GPT Index as a memory module"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "e06a04c1-c5fa-482c-b4d7-9b3fa0f904af",
            "metadata": {},
            "outputs": [],
            "source": [
                "# try using GPT List Index!\n",
                "from langchain import OpenAI\n",
                "\n",
                "from llama_index import ListIndex\n",
                "from llama_index.langchain_helpers.memory_wrapper import GPTIndexChatMemory"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "00e6694b-25fc-4fbc-8223-9a5605dc641f",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
                        "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 0 tokens\n",
                        "> [build_index_from_nodes] Total embedding token usage: 0 tokens\n"
                    ]
                }
            ],
            "source": [
                "index = ListIndex([])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "25c0b10c-bca4-49f1-9353-646a182050cf",
            "metadata": {},
            "outputs": [],
            "source": [
                "# set Logging to DEBUG for more detailed outputs\n",
                "# NOTE: you can also use a conversational chain\n",
                "\n",
                "memory = GPTIndexChatMemory(\n",
                "    index=index, \n",
                "    memory_key=\"chat_history\", \n",
                "    query_kwargs={\"response_mode\": \"compact\"},\n",
                "    # return_source returns source nodes instead of querying index\n",
                "    return_source=True,\n",
                "    # return_messages returns context in message format\n",
                "    return_messages=True\n",
                ")\n",
                "llm=OpenAI(temperature=0)\n",
                "agent_executor = initialize_agent([], llm, agent=\"conversational-react-description\", memory=memory)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "76193275-c47c-426c-b7e4-c54d31fda92d",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 0 tokens\n",
                        "> [get_response] Total LLM token usage: 0 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
                        "> [get_response] Total embedding token usage: 0 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
                        "> [insert] Total LLM token usage: 0 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 0 tokens\n",
                        "> [insert] Total embedding token usage: 0 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
                        "> [insert] Total LLM token usage: 0 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 0 tokens\n",
                        "> [insert] Total embedding token usage: 0 tokens\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "'Hi Bob, nice to meet you! How can I help you today?'"
                        ]
                    },
                    "execution_count": 18,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "agent_executor.run(input=\"hi, i am bob\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "id": "d426239a-a38b-4ae9-838b-b6fab43970e0",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 65 tokens\n",
                        "> [get_response] Total LLM token usage: 65 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
                        "> [get_response] Total embedding token usage: 0 tokens\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/sunhuawei/anaconda3/lib/python3.10/site-packages/llama_index/data_structs/node.py:182: UserWarning: .doc_id is deprecated, use .node.ref_doc_id instead\n",
                        "  warnings.warn(\".doc_id is deprecated, use .node.ref_doc_id instead\")\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
                        "> [insert] Total LLM token usage: 0 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 0 tokens\n",
                        "> [insert] Total embedding token usage: 0 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
                        "> [insert] Total LLM token usage: 0 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 0 tokens\n",
                        "> [insert] Total embedding token usage: 0 tokens\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "'Your name is Bob!'"
                        ]
                    },
                    "execution_count": 19,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# NOTE: the query now calls the ListIndex memory module. \n",
                "agent_executor.run(input=\"what's my name?\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "id": "d834e6f4",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'0fce5c89-a34b-408f-847c-dd1d8a14c68a': RefDocInfo(doc_ids=['83493872-2532-41ce-b64d-bc4f29243e61'], extra_info={}),\n",
                            " '43e6ef87-ad10-4801-bc7e-b1ef94169331': RefDocInfo(doc_ids=['87d0df48-3b98-4857-9219-2f7face60749'], extra_info={}),\n",
                            " '854d77bb-5000-49b7-b7c3-f2f649c9f803': RefDocInfo(doc_ids=['ea30f15b-a95d-423d-8b89-6b387a3a9559'], extra_info={}),\n",
                            " 'c72ef093-2b7f-4e62-87e4-6b54db3efa8e': RefDocInfo(doc_ids=['c9ea7b4a-9970-419e-bb2b-1c6fdb2cd9a4'], extra_info={})}"
                        ]
                    },
                    "execution_count": 22,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "index.ref_doc_info"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "id": "9e9c7220",
            "metadata": {},
            "outputs": [],
            "source": [
                "index.storage_context.persist(persist_dir='/media/sunhuawei/data/Projects/playground/data/storage/chat_history')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2abe91be",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
